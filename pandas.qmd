---
title: "Pandas"
subtitle: "Data wrangling essentials"
author: "Clemens Brunner"
date: 2022-05-17
format: html
toc: true
toc-title: ""
engine: knitr
highlight-style: github
title-block-banner: true
theme:
  light: [flatly, theme-light.scss]
  dark: [darkly, theme-dark.scss]
---

## Introduction

Why do we need another package on top of NumPy? Could we not just use NumPy arrays for our data analysis tasks? Unfortunately, in most cases this is not going to work. NumPy arrays are homogenous arrays, which means that all elements must have the same type. However, real-world tabular data generally consists of variables (columns) that have different types. For example, whereas some columns might contain integers, others might hold floating point numbers, text (strings), dates, or categorical data. Such a diverse collection of variables cannot be represented by a NumPy array[^1].

[^1]: NumPy supports arrays with different data types through [structured arrays](https://numpy.org/doc/stable/user/basics.rec.html), but pandas data frames are usually much easier to work with in a typical data analysis pipeline.

[Pandas](https://pandas.pydata.org/)[^2] offers a flexible data type for tabular data which builds upon NumPy arrays, but can accomodate heterogeneous column types: the so-called *data frame*.

[^2]: Note that pandas is spelled with all lower-case letters. Therefore, we will capitalize the first letter only at the beginning of a sentence in this document.

By convention, we import pandas as follows:

```{python}
import pandas as pd
```

Before we discuss data frames, we will introduce series as essential building blocks.

## Series
A pandas series (`pd.Series`) is a sequence-like homogeneous object, similar to a one-dimensional NumPy array. In fact, series are based on NumPy arrays under the hood, so they share important properties. For example, just like a NumPy array, a series has a specific data type (which means that all elements have the same type).

We can construct a series from any iterable, such as a standard list. Here's an example:

```{python}
pd.Series([1, 2, 3, 4, 5])
```

The output shows two columns: the values are in the right column, whereas the left column shows the index (which by default starts with zero). The bottom row displays the data type (`int64` in this case).

We can enforce a specific data type with the `dtype` argument:

```{python}
pd.Series([1, 2, 3, 4, 5], dtype="float64")
```

Pandas data types are based on [NumPy array scalars](https://numpy.org/doc/stable/reference/arrays.scalars.html). The most common numerical data types are:

- Integers: `int8`, `int16`, `int32`, and `int64`
- Unsigned integers: `uint8`, `uint16`, `uint32`, and `uint64`
- Floating point numbers: `float16`, `float32`, `float64`, and `float128`
- Complex numbers: `complex64`, `complex128`, and `complex256`

The shorthand notations `int`, `uint`, `float`, and `complex` map to `int64`, `uint64`, `float64`, and `complex128` on most machines, respectively. The numbers indicate the bits used to store one element of a particular type (for example, an `int16` value requires 16 bits of memory). Type specifications should be passed as a string when used as the `dtype` argument:

```{python}
pd.Series([1, 2, 3], dtype="int")
pd.Series([1, 2, 3], dtype="uint8")
pd.Series([1, 2, 3], dtype="float32")
pd.Series([1, 2, 3], dtype="complex256")
```

Strings are stored as `object` series by default (`object` is the base type in Python which every other type derives from â€“ so everything is an `object`, including strings).

```{python}
pd.Series(["a", "b", "c"])
```

Finally, boolean series are also supported:

```{python}
pd.Series([True, True, False])
```

Besides these basic data types, pandas also features so-called [extension data types](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes). Currently, the most important extension data types are:

- `category` for categorical data
- `Int8`, `Int16`, `Int32`, `Int64`, `UInt8`, `UInt16`, `UInt32`, and `UInt64` for integer types with support for missing values (note the upper case initial letters)
- `string` for string data
- `boolean` for boolean data with support for missing values (note that this is different from the basic `bool` data type)

```{python}
pd.Series(["a", "b", "c"], dtype="category")
pd.Series([1, 2, 3], dtype="Int64")
pd.Series(["a", "b", "c"], dtype="string")
pd.Series([True, True, False], dtype="boolean")
```

In general, it is recommended to use these extension data types because they perform as well as the core types, but offer better support for missing values or other functionality.

## Data frames
Pandas represents tabular data with a data frame, which is available as the `pd.DataFrame` type. It is basically a collection of columns, which correspond to individual `pd.Series` objects.

### Creating data frames
#### From arrays
Let's start with a simple example, which creates a data frame from a two-dimensional NumPy array:
```{python}
import numpy as np

data = np.arange(1, 21).reshape((4, 5))
pd.DataFrame(data)
```

The resulting data frame does not look very different from a NumPy array, except for row and column labels. However, keep in mind that each column could hold values with a different data type. In fact, each column is actually a `pd.Series` object.

We can supply custom column labels when creating a data frame:

```{python}
pd.DataFrame(data, columns=["A", "B", "C", "D", "E"])
```

Similarly, we could also set custom row labels with the `index` argument. However, this is rarely needed since consecutively numbered rows starting at zero are fine for most use cases.

#### Column-wise
Another way to generate a data frame is to pass a dictionary, where each key corresponds to a column label and the corresponding value is a sequence-like object containing the column elements:

```{python}
pd.DataFrame({"A": [1., 2, 3, 4], "B": ["w", "x", "y", "z"], "C": np.arange(4)})
```

In this example, each column has a different data type. The first column contains floating point numbers, the second column contains strings, and the third column holds integers.

:::{.callout-note}
The first element in the list `[1., 2, 3, 4]` is a float. Since all elements within a column must have the same type, all other values in that column are coerced to float as well.
:::

#### Row-wise
Sometimes, we might want to create a data frame row by row. We can use `pd.DataFrame.from_dict()` in combination with `orient="index"` for this purpose:

```{python}
pd.DataFrame.from_dict({1: [1, "a", 3, 5.], 2: [2, "b", 4, 6.]}, orient="index")
```

The dictionary keys are used for the row labels this time.

### Viewing data frames
Every data frame has useful attributes and methods that we can use to inspect it in more detail. Let's start with one of the example data frames we saw previously:

```{python}
df = pd.DataFrame(
    {"A": [1., 2, 3, 4], "B": ["w", "x", "y", "z"], "C": np.arange(4)}
)
df
```

We can access its row and column names with the `index` and `columns` attributes, respectively:

```{python}
df.index
df.columns
```

One of the most useful methods is `info()`, which displays essential information including the data type and number of non-missing values for each column:

```{python}
df.info()
```

In this example, we see that `df` consists of four rows and three columns. For each column, the label, number of non-missing (non-null) values, and data type are listed. Column A contains `float64` values (double precision floating point numbers), column B contains strings (available as `object`), and column C contains `int64` integers.

:::{.callout-note}
By default, pandas uses the `object` data type to represent strings. Newer versions also support a dedicated `string` type (see the [official documentation](https://pandas.pydata.org/docs/user_guide/text.html) for a comparison between `object` and `string` types).
:::

Obviously, typing `df` in an interactive interpreter prints the data frame:

```{python}
df
```

When the data frame has many rows and/or columns, the output only shows the first and last few rows and columns:

```{python}
import string  # needed to create a list of the 26 letters

df = pd.DataFrame(
    np.random.randint(-100, 100, (1000, 26)),
    columns=list(string.ascii_uppercase)  # custom column labels
)
df
```

We can change this behavior by setting the following two options to appropriate values (`None` means unlimited, whereas `0` uses the defaults):

```{python}
#| eval: false
pd.set_option("display.max_rows", None)  # print all rows
pd.set_option("display.max_columns", None)  # print all columns
```

This changes the display options *globally*, so from now on all data frames will use these display settings. However, we often want to change these settings only for a particular data frame. We can use a context manager for this purpose:

```{python}
with pd.option_context("display.max_rows", 4, "display.max_columns", 4):
    df
```

The modified display options are only valid within the context manager (the indented block of code).

Another way to inspect the first or last rows of a data frame is available with the `head()` and `tail()` methods, respectively:

```{python}
df.head()  # first 5 rows
df.tail()  # last 5 rows
df.head(10)  # first 10 rows
```

Finally, we can generate basic summary statistics for each column, which is especially useful for numeric columns:

```{python}
df.describe()
```

We can round the values in this table to fit more columns on the screen:

```{python}
df.describe().round(2)  # rounded to two decimal places
```

### Missing data
Almost any real-word data contains missing values. Pandas encodes missing numeric values as `NaN` ("Not a Number"), a sentinel value defined in the [IEEE 754 floating point standard](https://en.wikipedia.org/wiki/IEEE_754). Therefore, this missing value only exists for floating point data, and integer series are automatically converted to float if they contain missing values. Datetime types have a special `NaT` ("Not a Time") value to represent missing dates. Strings and boolean types can use `None` for missing values. In summary, missing data is represented with various different values depending on the underlying data type.

Recently, pandas has started to unify missing data support with the native `NA` ("Not Available") value (available as `pd.NA`). Future versions of pandas will likely support `NA` for all data types, but right now we have to live with the variety of different missing value representations (which of course are all handled correctly by pandas).

To illustrate these ideas, let's create a data frame with some missing values:

```{python}
df = pd.DataFrame(
    {"A": [1, None, 3], "B": [1., 2., None], "C": [True, False, None]}
)
df
df.dtypes
```

Notice that the first column is coerced to `float64` because it contains a missing value (otherwise, it would be `int64`). In both numerical columns, missing values are represented as `NaN`, whereas the last column is interpreted as a string (`object`) column because of the `None` element.

Luckily, it is easy to check which values are missing in a data frame with the `isna()` method:

```{python}
df.isna()
```

Most extension data types support the native `NA` value, which is displayed as `<NA>`:

```{python}
df = pd.DataFrame(
    {
        "A": pd.Series([1, pd.NA, 2], dtype="Int64"),
        "B": pd.Series([True, False, pd.NA], dtype="boolean"),
        "C": pd.Series([pd.NA, "b", "c"], dtype="string"),
        "D": [0.0, 0.5, 1.5],
        "E": [10, 11, 12]
    },
    index=[2, 0, 1]
)
df
df.dtypes
df.isna()
```

### Indexing
#### Basic indexing
Pandas data frames can be indexed to create subsets of the underlying data. Using square bracket notation, single columns can be indexed by their label:

```{python}
df["B"]
```

As we already know, this returns a pandas series.

:::{.callout-tip}
Instead of using square bracket notation, we can also index columns with dot notation, for example:

```{python}
df.B  # same as df["B"]
```

However, keep in mind that this only works for column labels that are also valid Python names. Square bracket notation always works.
:::

We can select multiple columns by specifying a list of column labels within the square brackets (which yields a smaller data frame):

```{python}
df[["A", "C"]]
```

The same notation can also be used for filtering rows. Instead of column labels, we have to provide a boolean list (or series), for example as generated by a comparison:

```{python}
df[df["A"] > 1]
df[[True, True, False]]  # rows 0 and 1
```

#### Indexing with `loc` and `iloc`
More sophisticated indexing is available via the `loc` and `iloc` attributes. Whereas `loc` uses row and column *labels*, `iloc` works with integer *indices* instead. Both variants support simultaneous row and column indexing (by passing a tuple with the desired row and column labels or indices, respectively).

Here are some examples showing `loc` in action (note that slices include the end point). First, we can grab a single column by its label:

```{python}
df.loc[:, "B"]
```

The same works for getting a single row with the index `1` (note that this refers to the index labeled `1` and not position `1`):

```{python}
df.loc[1, :]
```

We can even specify a range of columns (or rows). In contrast to almost everything else in Python, the end point of slices in `loc` are always *inclusive*. This example gets columns `B` through `D` (inclusive):

```{python}
df.loc[:, "B":"D"]
```

Combining row and column slices is also possible (again, note that we are referring to index and column *labels* here):

```{python}
df.loc[0:1, "B":"D"]
```

In contrast, `iloc` always refers to row and column positions instead of labels. Slices do not include the end point. Here are some examples:

```{python}
df.iloc[:, 1]  # column 1
df.iloc[0, :]  # row 0
df.iloc[:, 1:3]  # columns 1 and 2
df.iloc[:1, 1:3]  # row 0 and columns 1 and 2
```

## Importing data
Pandas supports importing data from a multitude of different formats. In this section, we will showcase how to import the most common data formats. In almost all cases, a successfully imported file will be available as a data frame.

### Text files
Text files are a popular option for storing data. Because text files are simple, they can be opened with any text editor and almost any tool you might use for data analysis. On the other hand, importing text files might involve a lot of parameter tweaking, because there is no universal standard on text file formatting (for example, text files can use arbitrary column separators, header rows, comments, decimal number formats, and so on). Moreover, text files store data inefficiently, which makes them a bad choice for very large data sets.

The function [`pd.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) is the main workhorse. It can import almost any kind of text data and features a large number of parameters that can be tuned to the idiosyncracies of a given file. Let's start with a simple text file named `data.csv` (located in the current working directory) with the following contents:

```
name,id,height,weight,gender,bp
Bob,4,177,83,m,93.2
Sue,2,179,,f,98.8
Alice,1,166,65,f,87.1
Hank,3,187,90,m,91
Tom,5,182,77,m,101.7
Mara,6,171,69,f,88.6
```

This looks pretty straightforward. The first row is a header which contains the column labels. The six columns are separated by `,` (commas), and there are six rows of data (not including the header). Note that the `weight` value in the second row is missing.

Importing this file is as simple as:

```{python}
df = pd.read_csv("data.csv")
df
```

Let's take a look at the inferred column data types:

```{python}
df.dtypes
```

Pandas uses the NumPy-based data types by default, so columns `name` and `gender` (which contain strings) are available as `object` columns. Columns `id` and `height` contain integers (`int64`), and column `bp` holds floating point numbers (`float64`). Although column `weight` originally contains integers, pandas converts it to `float64` because of the missing value.

If we are not happy with one of the automatically inferred data types, we can always change individual columns after importing with the `astype()` method. For example, the `gender` column could be represented with a categorical data type. We could also use the `Int64` data type for the `weight` column, which supports missing values. Finally, we could convert the `id` column from `object` to the more modern `string` data type. Note that we do not have to do any of these conversions, because the automatically inferred data types are fine in many cases.

```{python}
df["gender"] = df["gender"].astype("category")
df["weight"] = df["weight"].astype("Int64")
df["id"] = df["id"].astype("string")
df.dtypes
df
```

Alternatively, we can specify the desired data types when importing the file with the `dtype` argument. This is be more efficient especially if the data frame is large:

```{python}
df = pd.read_csv(
    "data.csv",
    dtype={"gender": "category", "weight": "Int64", "id": "string"}
)
df.dtypes
```

Let's now take a look at another text file (named `data.txt`) with the following contents:

```
Bob;4;177;83;m;93,2
Sue;2;179;;f;98,8
Alice;1;166;65;f;87,1
Hank;3;187;90;m;91
Tom;5;182;77;m;101,7
Mara;6;171;69;f;88,6
```

This file differs from the previous one in three ways:

1. There is no header row.
2. Columns are separated by semi-colons (`;`).
3. Decimal numbers use a comma (`,`) instead of a dot (`.`) as the decimal mark (which is common in German-speaking regions).

In order to import this file, we need to tweak some parameters, but let's first see what happens if we just read the file as before:

```{python}
pd.read_csv("data.txt")
```

As expected, this does not work, because the data frame consists of only two columns, and the column labels contain actual data. We can use the following arguments to adapt to the three changes we identified previously:

1. `header=None` specifies that there is no header row in the data file.
2. `sep=";"` sets the column separator.
3. `decimal=","` sets the decimal marker.

With these additional arguments, the resulting data frame looks much better:

```{python}
pd.read_csv("data.txt", header=None, sep=";", decimal=",")
```

The only thing missing are proper column names, which we can provide with the `names` argument:

```{python}
pd.read_csv(
    "data.txt",
    header=None,
    sep=";",
    decimal=",",
    names=["name", "id", "height", "weight", "gender", "bp"]
)
```

Of course, we could also specify custom column types as we did in our previous example. The `pd.read_csv()` function has many additional parameters that we do not have time to discuss. Therefore, I encourage you to consult the [`pd.read_csv()` reference](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) whenever you have problems importing a specific file. Chances are that many issues can be solved by setting a parameter to a suitable value.

:::{.callout-tip}
A quick way to create a toy data frame is to use `pd.read_csv()` in combination with a string with the contents of a ficticious CSV file. Since the function always expects a file name, we use `io.StringIO()` to imitate this behavior (we can read from the given string as if it was a file):

```{python}
from io import StringIO

pd.read_csv(StringIO("A,B,C\n1.,2,x\n4.,5,y\n7.,8,z"))
```
:::

Other useful functions for reading text data include `pd.read_clipboard()`, `pd.read_fwf()`, `pd.read_json()`, and `pd.read_xml()`. Consult the [pandas documentation](https://pandas.pydata.org/docs/user_guide/io.html) to learn more about them.

### Excel files
Like it or not, Excel is still a popular tool for analyzing (small) data sets. It is also common as a data storage format (`.xlsx`). Pandas can import Excel sheets with `pd.read_excel()` (which also supports the OpenDocument format `.ods` used by LibreOffice).

:::{.callout-important}
To import Excel files with pandas, you need to install the following packages depending on the file format:

- `openpyxl` for modern Excel files (.xlsx)
- `xlrd` for older Excel files (.xls)
- `odfpy` for OpenDocument files (.ods)

It is generally a good idea to install all three dependencies if you are planning to work with Excel files.
:::

Let's import data from an Excel file named `data.xlsx` (located in the current working directory), which contains the same table we saw in the previous section:

```{python}
pd.read_excel("data.xlsx")
```

We can use many of the arguments we already know from `pd.read_csv()` (such as `header` and `dtype`). In addition, Excel files can contain multiple sheets. To specify which sheet you want to import, use the `sheet_name` parameter. For more information on all available parameters, consult the [`pd.read_excel()` reference](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html).

### SPSS/Stata/SAS files
Similar to Excel files, pandas can import data stored in SPSS, Stata, and SAS files with dedicated reader functions [`pd.read_spss()`](https://pandas.pydata.org/docs/reference/api/pandas.read_spss.html), [`pd.read_stata()`](https://pandas.pydata.org/docs/reference/api/pandas.read_stata.html), and [`pd.read_sas()`](https://pandas.pydata.org/docs/reference/api/pandas.read_sas.html).

:::{.callout-important}
Reading SPSS, Stata, and SAS files requires the `pyreadstat` package.
:::

## Data wrangling
Data wrangling can be broken down into the following five tasks (as defined in [R for Data Science](https://r4ds.had.co.nz/transform.html#dplyr-basics)):

-   Filtering observations (rows)
-   Arranging (reordering) rows
-   Selecting variables (columns)
-   Creating new variables (columns) based on existing ones
-   Summarizing data, typically in combination with grouping data

We will illustrate each operation using the `flights` data frame from the [nycflights13](https://github.com/tidyverse/nycflights13) data set. This data frame is available as a zipped CSV file in the current working directory, and we can import it as follows:

```{python}
flights = pd.read_csv(
    "flights.zip",
    dtype={
        "dep_time": "Int64",
        "dep_delay": "Int64",
        "arr_time": "Int64",
        "arr_delay": "Int64",
        "air_time": "Int64",
        "carrier": "string",
        "tailnum": "string",
        "origin": "string",
        "dest": "string"
    }
)
flights.drop(columns=["year", "hour", "minute", "time_hour"], inplace=True)
flights.info()
```

Notice that in order to get better support for missing values, we specify the data types for integer and string columns manually as one of the extension types `Int64` and `string`, respectively. We also drop the `year`, `hour`, `minute`, and `time_hour` columns because we won't need them in our analyses (and they contain only redundant information anyway).

The `nycflights13` data set contains data on all 336,776 flights departing from New York City airports ([JFK](https://en.wikipedia.org/wiki/John_F._Kennedy_International_Airport), [LGA](https://en.wikipedia.org/wiki/LaGuardia_Airport), and [EWR](https://en.wikipedia.org/wiki/Newark_Liberty_International_Airport)) in 2013. Let's see what we can find out about that data set.

### Filtering observations
First, we might want to know how many flights departed on New Year's Day 2013. Each flight is represented by a row, so we need to filter rows with flights that departed on January 1 (columns `month` and `day` contain the dates).

The `query()` method filters rows according to a specified condition based on values in one or more columns. The query must be passed as a single string, which may contain column names without having to use any special indexing syntax. The query returns a new data frame containing only the filtered rows, which in our example consists of 842 rows:

```{python}
flights.query("month == 1 and day == 1")
```

Notice that we combined two conditions with `and`; we can also use `or`, for example to filter flights that departed in November or December:

```{python}
flights.query("month == 11 or month == 12")
```

The `air_time` column contains the flight time in minutes. Are there any flights with a duration of less than 25 minutes?

```{python}
flights.query("air_time < 25")
```

What about flights that made up time? In other words, how many flights had a smaller arrival delay than their corresponding departure delay?

```{python}
flights.query("arr_delay < dep_delay")
```

:::{.callout-note}
Instead of `query()`, we can also filter rows with indexing. For example, flights with an air time of less than 25 minutes can be obtained with:

```{python}
flights[flights["air_time"] < 25]
```

However, in most cases this is more verbose than `query()`, especially when combining several conditions.
:::

Filtering rows implicitly drops missing values that are present in the column(s) of interest. To explicitly drop missing values, we can use the `dropna()` method. The following command drops all rows containing a missing value in any column:

```{python}
flights.dropna()
```

If we only want to drop rows with missing values in one (or more) specific columns, we can use the `subset` argument. Here's how to drop all rows that have missing values in the `air_time` column:

```{python}
flights.dropna(subset="air_time")
```

:::{.callout-note}
The `dropna()` method does *not* modify the original data frame by default (instead, it returns a new data frame). This can be changed by setting `inplace=True`.
:::

### Arranging rows
Another frequently used operation is sorting rows in a data frame according to values in one or more columns. The `sort_values()` method accomplishes this task, by default is arranges rows in ascending order. We can use this method to sort all flights from first to last scheduled departure time within the entire year as follows:

```{python}
flights.sort_values(["month", "day", "sched_dep_time"])
```

Which flights had the larges departure delay? To answer this question, we can sort by the `dep_delay` column in descending order (so that flights with the largest delays are on top):

```{python}
flights.sort_values("dep_delay", ascending=False)
```

:::{.callout-note}
Missing values will *always* be sorted at the end (for both ascending and descending order).
:::

Unfortunately, the `dep_delay` column is not visible in the output, but we can select (and therefore show) this column with indexing:

```{python}
flights.sort_values("dep_delay", ascending=False)["dep_delay"]
```

1301 minutes are almost 22 hours! That's a long delay for sure!

### Selecting variables
Next, we will learn how to select specific columns with the `filter()` method. Let's start with a basic example, where we pick two columns passed as a list of column labels:

```{python}
flights.filter(["month", "day"])
```

We can also pick columns with [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) (using the `regex` argument). For example, we can pick all columns containing `delay` as follows:

```{python}
flights.filter(regex="delay")
```

Or what about columns that contain the string `arr_`?

```{python}
flights.filter(regex="arr_")
```

The next example selects all columns starting with `a`:

```{python}
flights.filter(regex="^a")
```

Finally, we pick all columns ending with `time`:

```{python}
flights.filter(regex="time$")
```

Regular expressions are a powerful way to describe text patterns. We do not have time to go into any more details, but it pays off to learn at least the basic concepts (for example with [this tutorial](https://regexone.com/)).

### Creating new variables
First, let's create a smaller data frame with only a few columns. This will make it easier to see the new columns we will add in this section.

```{python}
df = flights.filter(["dep_delay", "arr_delay", "distance", "air_time"])
```

Now imagine we wanted to add a new column containing the gained time of each flight, which can be computed as the difference between arrival delay and departure delay. If the gain is positive, the airplane was faster than planned and therefore gained time.

Pandas offers the `assign()` method to compute new columns. Let's add a `gain` column we just discussed, and in addition let's also compute the average speed of each flight (in kilometers per hour) using the existing `air_time` and `distance` columns:

```{python}
df.assign(
    gain=df["arr_delay"] - df["dep_delay"],
    hours=df["air_time"] / 60,
    speed=lambda x: x["distance"] / x["hours"]
)
```

Notice that we can use columns that we just created to create another new variable like `speed`, where we use the `hours` column which we created in the previous line (but in the same method call). We do have to use a lambda function though, because `df["hours"]` does not exist in the original data frame.

### Summarizing data
Summarizing data can yield valuable insights, especially when performed on grouped data. In most cases, our goal is to summarize or aggregate values from a single column. Each column is represented by a `pd.Series`, and we can use the `agg()` method to summarize its values with a given aggregation function. An aggregation function can be any function which returns exactly one value (a scalar) such as the mean, median, standard deviation, and so on.

Here's an example which summarizes the `dep_delay` column by its mean:

```{python}
flights["dep_delay"].agg("mean")
```

We could also select the column with the `filter()` method we saw before:

```{python}
flights.filter(["dep_delay"]).agg("mean")
```

Missing values are automatically removed before `agg()` calculates the summary statistic.

:::{.callout-note}
The first example `flights["dep_delay"].agg("mean")` returns a scalar, whereas the second example `flights.filter(["dep_delay"]).agg("mean")` returns the value inside a `pd.Series`. The reason for this difference is that the `filter()` method always returns a data frame, even if it consists of only one column. Indexing with square bracket notation returns a `pd.Series` if we are asking for only one column.
:::

We can even compute several summary statistics simulteneously by passing a list of functions:

```{python}
flights.filter(["dep_delay"]).agg(["mean", "std", "min", "max"])
```

The most important aggregation functions are available as strings (as in the previous example), but in general we can pass an arbitrary function which takes a sequence-like object and returns a single value. For example, we could write the previous mean aggregation as:

```{python}
flights["dep_delay"].agg(np.mean)
```

We could also provide our own function:

```{python}
flights["dep_delay"].agg(lambda x: sum(x.dropna()) / len(x.dropna()))
```

For even moderately complex functions, lambda functions tend to get unreadable, so you should prefer a regular (named) function instead.

Things get more interesting when we group the data first and aggregate within each group individually. The `groupby()` data frame method groups the data into groups based on one or more columns. For example, we can group by month and day to calculate the mean departure delay on each single day:

```{python}
flights.groupby(["month", "day"])["dep_delay"].agg("mean")
```

Similarly, here's the average departure delay per month:

```{python}
flights.groupby("month")["dep_delay"].agg("mean")
```

Notice how we select the `dep_delay` column of the grouped data frame with square bracket indexing. Another option is to specify the columns that should be aggregated in the argument of the `agg()` method as follows:

```{python}
flights.groupby("month").agg({"dep_delay": "mean"})
```

:::{.callout-note}
Do you see the difference in the outputs of the previous two examples? The first output is a `pd.Series`, whereas the second output is a `pd.Dataframe` with one column.
:::

The `count()` method counts the number of non-missing values in a column:

```{python}
flights.groupby("month").agg({"dep_delay": "count"})
```

In contrast, the number of missing values in that column can be obtained with:

```{python}
flights.groupby("month")["dep_delay"].agg(lambda x: x.isna().sum())
```

The `value_counts()` method determines the number of occurrences for each unique value in a column. For example, an individual airplane is identified by its tail number. If we want to know how many unique airplanes there are in the data set, and how many flights each individual airplane had, we can use the following command:

```{python}
flights["tailnum"].value_counts()
```

This output shows that there are 4043 unique airplanes, each of which is listed with its corresponding number of flights. The 10 airplanes with the most flights are:

```{python}
flights["tailnum"].value_counts().head(10)
```

How many airplanes have just one flight?

```{python}
(flights["tailnum"].value_counts() == 1).sum()
```

Here, we exploited the fact that summing a boolean series counts `True` as 1 and `False` as 0.

### Pipelines
Almost all interesting data wrangling tasks require a combination of the five previously mentioned basic operations. You might have already noticed this in some examples where we first picked a subset of columns and then performed some operations (like adding new variables). Usually, this requires multiple steps involving temporary names, for example:

```{python}
df1 = flights.filter(["dep_delay", "arr_delay", "distance", "air_time"])
df2 = df1.assign(
    gain=df["arr_delay"] - df["dep_delay"],
    hours=df["air_time"] / 60,
    speed=lambda x: x["distance"] / x["hours"]
)
df2.sort_values("speed")
```

The problem here is that we do not really need the names `df1` and `df2` other than to perform the desired operations sequentially. Luckily, we can use a so-called pipeline to combine two or more operations without the need to create temporary names:

```{python}
(flights
    .filter(["dep_delay", "arr_delay", "distance", "air_time"])
    .assign(
        gain=df["arr_delay"] - df["dep_delay"],
        hours=df["air_time"] / 60,
        speed=lambda x: x["distance"] / x["hours"]
    )
    .sort_values("speed", ascending=False)
)
```

:::{.callout-important}
Breaking up a pipeline into multiple rows (as shown in the previous example) creates a nicely formatted representation of what happens step by step. However, this requires enclosing the entire pipleline in parentheses.
:::

## Tidy data
There are many ways to organize data into rows and columns, but the [tidy data](https://www.jstatsoft.org/article/view/v059i10) format has some compelling advantages. A table is tidy when

-   each variable corresponds to exactly one column,
-   each observation corresponds to exactly one row,
-   and each value corresponds to exactly one cell.

> "Tidy datasets are all alike, but every messy dataset is messy in its own way." -- Hadley Wickham

Consider the following table (taken from the `tidyr` R package):

| country     | year |  cases | population |
|-------------|-----:|-------:|-----------:|
| Afghanistan | 1999 |    745 |   19987071 |
| Afghanistan | 2000 |   2666 |   20595360 |
| Brazil      | 1999 |  37737 |  172006362 |
| Brazil      | 2000 |  80488 |  174504898 |
| China       | 1999 | 212258 | 1272915272 |
| China       | 2000 | 213766 | 1280428583 |

: A tidy table. {#tbl-tidy}

This table is tidy because it meets all three criteria. It might be difficult to see why this is the case, so let's look at other tables (containing the same data) that are *not* tidy. The following table combines two variables into one column, which makes the table twice as long:

| country     | year | type       |      count |
|-------------|-----:|------------|-----------:|
| Afghanistan | 1999 | cases      |        745 |
| Afghanistan | 1999 | population |   19987071 |
| Afghanistan | 2000 | cases      |       2666 |
| Afghanistan | 2000 | population |   20595360 |
| Brazil      | 1999 | cases      |      37737 |
| Brazil      | 1999 | population |  172006362 |
| Brazil      | 2000 | cases      |      80488 |
| Brazil      | 2000 | population |  174504898 |
| China       | 1999 | cases      |     212258 |
| China       | 1999 | population | 1272915272 |
| China       | 2000 | cases      |     213766 |
| China       | 2000 | population | 1280428583 |

: Variables "cases" and "population" are not in two separate columns. {#tbl-messy-1}

For the same reason, the next table is also not tidy. It contains a combination of two variables in one column. In addition, this column is not numeric, which makes it difficult to work with the actual data.

| country     | year | rate              |
|-------------|-----:|-------------------|
| Afghanistan | 1999 | 745/19987071      |
| Afghanistan | 2000 | 2666/20595360     |
| Brazil      | 1999 | 37737/172006362   |
| Brazil      | 2000 | 80488/174504898   |
| China       | 1999 | 212258/1272915272 |
| China       | 2000 | 213766/1280428583 |

: Variables "cases" and "population" are combined in one column. {#tbl-messy-2}

In pandas, working with tidy data greatly facilitates many operations, so whenever possible this should be the preferred table format. However, depending on the task, this might not always be possible or practical, so it is completely fine if another data organization works better.

Pandas makes it pretty straightforward to convert any given table into a tidy format (or vice versa). Each data frame has a `pivot()` method for this task. Let's start with data from @tbl-messy-1 and use `pivot()` to make it tidy:

```{python}
messy1 = pd.read_csv(StringIO("""country,year,type,count
Afghanistan,1999,cases,745
Afghanistan,1999,population,19987071
Afghanistan,2000,cases,2666
Afghanistan,2000,population,20595360
Brazil,1999,cases,37737
Brazil,1999,population,172006362
Brazil,2000,cases,80488
Brazil,2000,population,174504898
China,1999,cases,212258
China,1999,population,1272915272
China,2000,cases,213766
China,2000,population,1280428583"""))
```

The `index` argument specifies which columns should be moved to new rows (the index), whereas the `columns` argument specifies which columns should be split up into new columns. Finally, the (optional) `values` argument lists the columns containing the values of the new data frame:

```{python}
messy1.pivot(index=["country", "year"], columns="type", values="count")
```

This result is a data frame consisting of two columns `cases` and `population` and an index consisting of `country` and `year` (a so-called multiindex). We can convert a multiindex into separate columns using the `reset_index()` method (which resets the index to the default integer series starting at zero):

```{python}
(messy1
    .pivot(index=["country", "year"], columns="type", values="count")
    .reset_index()
)
```

The `type` in the output is the name of the columns (which is usually empty). If this bothers you, the column name attribute can be set to `None` (but we cannot do this inside a pipeline):

```{python}
tidy1 = (messy1
    .pivot(index=["country", "year"], columns="type", values="count")
    .reset_index()
)
tidy1.columns.name = None
tidy1
```

Let's try to tidy the data from @tbl-messy-2:

```{python}
messy2 = pd.read_csv(StringIO("""country,year,rate
Afghanistan,1999,745/19987071
Afghanistan,2000,2666/20595360
Brazil,1999,37737/172006362
Brazil,2000,80488/174504898
China,1999,212258/1272915272
China,2000,213766/1280428583"""))
```

The problem here is that two variables are combined into one column `rate`, which is of type `object`:

```{python}
messy2.info()
```

String columns offer a lot of useful methods such as slicing or splitting using the `str` method namespace. In our example, we would like to split the column on the `/` character, resulting in two new columns:

```{python}
messy2["rate"].str.split("/", expand=True)
```

Note that this operation creates a new data frame because we set `expand=True` (by default, `expand=False` creates a series). We can add this resulting data frame to our tidy result:

```{python}
tidy2 = messy2.loc[:, ["country", "year"]]
tidy2[["count", "population"]] = messy2["rate"].str.split("/", expand=True)
```

Now we are almost done, the data frame is tidy, but the columns types of `count` and `population` are still not numeric (they are `object` just like the original column). We need to explicitly convert them, for example with `pd.to_numeric()`:

```{python}
tidy2["count"] = pd.to_numeric(tidy2["count"])
tidy2["population"] = pd.to_numeric(tidy2["population"])
tidy2
```

## COVID-19 example
Let's apply some of the things we learned in this chapter to a real-world data set: official Covid-19 cases in Austria. The data is available as a text file at https://covid19-dashboard.ages.at/data/CovidFaelle_Timeline.csv (updated daily), and a snapshot from June 15 2022 (renamed to `covid19.csv`) is located in the current working directory.

Using `pd.read_csv()`, we can import this file as follows:

```{python}
covid19 = pd.read_csv(
    "covid19.csv",
    sep=";",
    decimal=",",
    parse_dates=["Time"],
    dayfirst=True
)
```

The file uses semicolons (`;`) to separate columns and commas (`,`) for decimal numbers, which we need to specify in order to correctly import the data. Passing `parse_dates=["Time"]` together with `dayfirst=True` makes sure that the `Time` column has the suitable datetime data type. If you are wondering where this information is available â€“ the only way to find out for sure is to open the file in a text editor and inspect the values!

:::{.callout-note}
If we do not specify `decimal=","`, pandas expects a point (`.`) as the decimal separator. If we then import a column with a different decimal separator, it will be interpreted as type `object`. Therefore, it is important to carefully check all column types after importing (and verify that numeric columns have numeric types).
:::

There are 8410 rows and 12 columns with no missing values, and the column data types have been inferred correctly (the first column is a datetime, and all remaining columns are numeric except for `Bundesland`, which contains the names of the Austrian provinces):

```{python}
covid19.info()
```

There are many interesting questions we could try to answer with this data set. Let's highlight a few simple ones that each introduce a new feature that we have not seen before.

First, because Covid-19 infections heavily depend on the seasibs, we could be interested in average daily new cases per month. A datetime series exposes specialized method in its `dt` method namespace. Here, we use `dt.month` to extract the month of the year (1â€“12) and add this information as a new column `month`. Then we group by `month` and compute the average in each group:

```{python}
(covid19
    .assign(month=covid19["Time"].dt.month)
    .groupby("month")
    .agg({"AnzahlFaelle": "mean"})
)
```

Similarly, we know that new cases also depend on the day of the week, because fewer tests are administered on the weekend. Therefore, the average cases aggregated per weekday are (0â€“6 correspond to Mondayâ€“Sunday):

```{python}
(covid19
    .assign(weekday=covid19["Time"].dt.weekday)
    .groupby("weekday")
    .agg({"AnzahlFaelle": "mean"})
)
```

We can also create plots directly from dataframes and/or single columns. That way, we can recreate the famous time course of daily Covid-19 infections over the whole time period. We will plot the daily values for Austria with "Time" on the *x*-axis and new cases on the *y*-axis:

```{python}
#| eval: false
(covid19
    .query("Bundesland == 'Ã–sterreich'")
    .set_index("Time")
    ["AnzahlFaelle"]
    .plot()
)
```

```{python}
#| echo: false
#| classes: dark-mode
import matplotlib.pyplot as plt
plt.style.use("dark_background")
ax = (covid19
    .query("Bundesland == 'Ã–sterreich'")
    .set_index("Time")
    ["AnzahlFaelle"]
    .plot()
)
fig = ax.get_figure()
fig.set_tight_layout(True)
fig.patch.set_alpha(0)
plt.show()
```

```{python}
#| echo: false
#| classes: light-mode
import matplotlib.pyplot as plt
plt.style.use("default")
ax = (covid19
    .query("Bundesland == 'Ã–sterreich'")
    .set_index("Time")
    ["AnzahlFaelle"]
    .plot()
)
fig = ax.get_figure()
fig.set_tight_layout(True)
fig.patch.set_alpha(0)
plt.show()
```

Note that when we plot a single column, the index will be displayed on the *x*-axis. That's why we explicitly set the index to the `Time` column (which means that the time column gets "promoted" to the index). After that, we pick the column of interest and call its `plot()` method to generate the visualization.

---

Â© [Clemens Brunner](https://cbrnr.github.io/) ([CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/))
